geom_tile(width = 0.8, height = 0.8, show.legend = FALSE) +
geom_text(aes(label = Cluster), color = "white", fontface = "bold", size = 4) +
geom_text(aes(x = 2, label = Zona), color = "black", hjust = 0, size = 4) +
scale_fill_manual(values = clasificacion$Color) +
theme_void() +
coord_cartesian(clip = "off") +
xlim(0.5, 3.5)
cluster_serv <- read.csv("cluster.csv")
fotocasa$cluster_serv <- cluster_serv$cluster
fotocasa$cluster_serv <- factor(fotocasa$cluster_serv,
levels = c(1, 2, 3, 4, 5, 6, 7),
labels = c("Poblats marítims", "Centro historico-Pedanias-Z. universitaria", "Ruzafa-Jesus-Benicalap", "Periferia", "Aragón-Blasco Íbañez", "Gran Vía-Colón-Z. universitaria","Tarongers"))
g1 <- fviz_pca_biplot(res.pca, axes = c(1, 2), repel = TRUE,select.var = list(contrib = 15),
col.ind = fotocasa$cluster_serv,
col.var = "black",
label = "var",
pointsize = 0.8) +   # puntos más pequeños
labs(title = "PCA: 1 vs 2") +
theme_minimal(base_size = 9)
g3 <- fviz_pca_biplot(res.pca, axes = c(1,3), repel = TRUE,select.var = list(contrib = 15),
col.ind = fotocasa$cluster_serv,
col.var = "black",
label = "var",
pointsize = 0.8) +
labs(title = "PCA: 1 vs 3") +
theme_minimal(base_size = 9)
g1
#g3
#grid.arrange(g1, g2, ncol = 2)
# Supón que quieres copiar la columna "mi_columna" del dataframe df1
write.csv(fotocasaServicios["cluster"], "cluster.csv", row.names = FALSE)
afc_fotocasa = read_excel("data_clean1.xlsx")
# Diccionario para traducir los códigos de propertySubtypeId a nombres descriptivos
subtype_map <- c("1" = "Piso",
"2" = "Apartamento",
"3" = "Casa o chalet",
"5" = "Casa adosada",
"6" = "Ático",
"7" = "Dúplex",
"8" = "Loft",
"52" = "Bajos",
"54" = "Estudio",
"10" = "Otros")
# Aplicar el mapeo
afc_fotocasa$propertySubtypeCat <- subtype_map[as.character(afc_fotocasa$propertySubtypeId)]
# Crear categorías ordinales de precio con cuantiles
afc_fotocasa$priceCategory <- cut(afc_fotocasa$priceAmount,
breaks = quantile(afc_fotocasa$priceAmount, probs = seq(0, 1, length.out = 7), na.rm = TRUE),
include.lowest = TRUE,
labels = c("Muy Bajo", "Bajo", "Medio Bajo", "Medio Alto", "Alto", "Muy Alto")
)
# Tabla de contingencia Subtipo vs Categoría de Precio
tabla_subtipo_precio <- table(afc_fotocasa$propertySubtypeCat, afc_fotocasa$priceCategory)
# Mostrar tabla
tabla_subtipo_precio
# Test Chi-cuadrado para evaluar independencia
chisq.test(tabla_subtipo_precio, simulate.p.value = TRUE)
afc_subtipo = CA(tabla_subtipo_precio, graph = FALSE)
# Inercia explicada por dimensión
valores_propios_subtipo = get_eigenvalue(afc_subtipo)
# Inercia media
inercia_media_subtipo = 100 * (1 / nrow(valores_propios_subtipo))
# Gráfico de varianza explicada
fviz_eig(afc_subtipo, addlabels = TRUE) +
geom_hline(yintercept = inercia_media_subtipo, linetype = 2, color = "red") +
ggtitle("Varianza Explicada por Componente")
# Análisis de correspondencias
afc_subtipo = CA(tabla_subtipo_precio, graph = FALSE, ncp = 2)
# Gráfico de subtipos de vivienda
g1 <- fviz_ca_row(afc_subtipo, axes = c(1, 2), repel = TRUE,
col.row = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
title = "Subtipos de Vivienda")
# Gráfico de categorías de precio
g2 <- fviz_ca_col(afc_subtipo, axes = c(1, 2), repel = TRUE,
col.col = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
title = "Categorías de Precio")
# Mostrar ambos juntos
grid.arrange(g1, g2, ncol = 2)
datosReglas <- read_excel("fotocasaImp.xlsx")
data_variables <- datosReglas %>%
dplyr::select(URL, bathrooms, rooms, surface, priceAmount,
tieneAscensor, tieneTrastero, tieneCalefaccion, tieneAireAcondicionado)
# Convertir variables
data_variables$tieneAscensor <- ifelse(data_variables$tieneAscensor == 1, "con_ascensor", "sin_ascensor")
data_variables$tieneTrastero <- ifelse(data_variables$tieneTrastero == 1, "con_trastero", "sin_trastero")
data_variables$tieneCalefaccion <- ifelse(data_variables$tieneCalefaccion == 1, "con_calefaccion", "sin_calefaccion")
data_variables$tieneAireAcondicionado <- ifelse(data_variables$tieneAireAcondicionado == 1, "con_aire", "sin_aire")
data_variables$bathrooms <- dplyr::case_when(
data_variables$bathrooms == 1 ~ "1_banio",
data_variables$bathrooms == 2 ~ "2_banios",
data_variables$bathrooms >= 3 ~ "3+_banios",
TRUE ~ NA_character_
)
data_variables$rooms <- dplyr::case_when(
data_variables$rooms == 1 ~ "1_habitacion",
data_variables$rooms == 2 ~ "2_habitaciones",
data_variables$rooms == 3 ~ "3_habitaciones",
data_variables$rooms >= 4 ~ "4+_habitaciones",
TRUE ~ NA_character_
)
# Crear variable categórica de precio en cuartiles
cuartiles <- ntile(data_variables$priceAmount, 4)
data_variables$priceAmount <- dplyr::case_when(
cuartiles == 1 ~ "precio_bajo",
cuartiles == 2 ~ "precio_medio_bajo",
cuartiles == 3 ~ "precio_medio_alto",
cuartiles == 4 ~ "precio_alto"
)
# Crear variable categórica de surface en cuartiles
cuartiles_surface <- ntile(data_variables$surface, 3)
data_variables$surface <- dplyr::case_when(
cuartiles_surface == 1 ~ "surface_bajo",
cuartiles_surface == 2 ~ "surface_medio",
cuartiles_surface == 3 ~ "surface_alto"
)
# Convertir a formato largo
datos_largos <- data_variables %>%
dplyr::select(-URL) %>%  # Elimina columnas no categóricas o redundantes
mutate_all(as.character) %>%
mutate(id = row_number()) %>%
pivot_longer(cols = -id, names_to = "atributo", values_to = "items") %>%
unite("items", atributo, items, sep = "_")
# Crear tabla binaria
tabla_binaria <- datos_largos %>%
mutate(valor = 1) %>%
pivot_wider(names_from = items, values_from = valor, values_fill = 0)
# Convertir a transacciones
transacciones <- as(as.matrix(tabla_binaria[,-1]), "transactions")
reglas <- apriori(transacciones, parameter = list(supp = 0.01, conf = 0.5))
reglas <- reglas[!is.redundant(reglas)]
reglas_filtradas <- subset(reglas, confidence > 0.7 & lift > 3.5 & support > 0.015)
reglas_ordenadas <- sort(reglas_filtradas, by = "lift", decreasing = TRUE)
reglas_df <- as(reglas_ordenadas[1:5], "data.frame")
reglas_filtradas_precio_bajo <- subset(reglas, rhs %in% "priceAmount_precio_bajo" & lift>2.7 & confidence>0.8 & support>0.0107)
reglas_bajas_ordenadas <- sort(reglas_filtradas_precio_bajo, by = "lift", decreasing = TRUE)
reglas_df_bajas <- as(reglas_bajas_ordenadas, "data.frame")
kable(reglas_df, format = "latex", digits = 3,
caption = "Reglas de asociación más relevantes (ordenadas por lift)") %>%
kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"))
kable(reglas_df_bajas, format = "latex", digits = 3,
caption = "Reglas de asociación más relevantes asociadas a Precios Bajos (ordenadas por lift)") %>%
kableExtra::kable_styling(latex_options = c("scale_down", "hold_position"))
# Gráfico 1: Dispersión Soporte vs ConfiXanza
plot(reglas_filtradas, method = "scatterplot", shading = "confidence")
# Gráfico 2: Coordenadas paralelas
par(mar = c(4, 7, 3, 1))  # Márgenes ajustados
plot(reglas_filtradas, method = "paracoord", shading = "confidence")
fotocasa = read_excel("fotocasaImp.xlsx")
fotocasa$priceDropBinary <- ifelse(fotocasa$priceAmountDrop > 0, 1, 0)
cluster_serv <- read.csv("cluster.csv")
fotocasa$cluster <- cluster_serv$cluster
fotocasa_original=fotocasa
# Limpiar y seleccionar variables relevantes (modifica según variables de tu dataset)
fotocasa <- fotocasa[, !(names(fotocasa) %in% c("URL", "lat", "lng", "municipality", "neighborhood", "zipCode", "priceAmountDrop", "creationDate"))]
# Tabla original
tabla_ini <- fotocasa_original %>%
count(priceDropBinary) %>%
mutate(Etapa = "Original",
porcentaje = round(100 * n / sum(n), 1))
# Gráfico original
g1 <- ggplot(tabla_ini, aes(x = as.factor(priceDropBinary), y = n, fill = as.factor(priceDropBinary))) +
geom_col(width = 0.6) +
geom_text(aes(label = paste0(porcentaje, "%")), vjust = -0.5, size = 2) +
scale_fill_manual(values = c("0" = "#6BAED6", "1" = "#FF55C7")) +
labs(title = "Distribución original",
x = "Clase (priceDropBinary)", y = "Frecuencia") +
ylim(0, max(tabla_ini$n) * 1.1) +
theme_minimal(base_size = 7) +
theme(legend.position = "none")
# --- División inicial ---
set.seed(456)
train_indices <- sample(1:nrow(fotocasa), size = 0.7 * nrow(fotocasa))
fotocasa_train <- fotocasa[train_indices, ]
fotocasa_test  <- fotocasa[-train_indices, ]
# --- Undersampling SOLO al entrenamiento ---
fotocasa_train_0 <- filter(fotocasa_train, priceDropBinary == 0)
fotocasa_train_1 <- filter(fotocasa_train, priceDropBinary == 1)
n1 <- nrow(fotocasa_train_1)
n0_target <- n1  # misma cantidad que clase 1
set.seed(456)
fotocasa_train_bal <- bind_rows(sample_n(fotocasa_train_0, n0_target), fotocasa_train_1)
# Tabla tras muestreo
tabla_fin <- fotocasa_train_bal %>%
count(priceDropBinary) %>%
mutate(Etapa = "Tras muestreo",
porcentaje = round(100 * n / sum(n), 1))
# Gráfico tras muestreo
g2 <- ggplot(tabla_fin, aes(x = as.factor(priceDropBinary), y = n, fill = as.factor(priceDropBinary))) +
geom_col(width = 0.6) +
geom_text(aes(label = paste0(porcentaje, "%")), vjust = -0.5, size =2) +
scale_fill_manual(values = c("0" = "#6BAED6", "1" = "#FF55C7")) +
labs(title = "Distribución tras muestreo",
x = "Clase (priceDropBinary)", y = "Frecuencia") +
ylim(0, max(tabla_ini$n, tabla_fin$n) * 1.1) +
theme_minimal(base_size = 7) +
theme(legend.position = "none")
# --- Combinar para model.matrix ---
fotocasa_train_bal$set <- "train"
fotocasa_test$set <- "test"
fotocasa_both <- bind_rows(fotocasa_train_bal, fotocasa_test)
# Variables como factor
fotocasa_both <- fotocasa_both %>%
mutate(
cluster = as.factor(cluster),
propertySubtypeId = as.factor(propertySubtypeId),
ownerType = as.factor(ownerType),
tieneCalefaccion = as.factor(tieneCalefaccion),
tieneTrastero = as.factor(tieneTrastero),
tieneAscensor = as.factor(tieneAscensor),
hotWater = as.factor(hotWater),
tieneAireAcondicionado = as.factor(tieneAireAcondicionado)
)
X_data <- dplyr::select(fotocasa_both, -priceDropBinary, -set)
X_all <- model.matrix(~ . - 1, data = X_data) %>% scale()
# Separar nuevamente
X_train <- X_all[fotocasa_both$set == "train", ]
X_test  <- X_all[fotocasa_both$set == "test", ]
Y_train <- as.factor(fotocasa_train_bal$priceDropBinary)
Y_test  <- as.factor(fotocasa_test$priceDropBinary)
# Mostrar gráficos
g1 + g2
fotocasa$cluster <- as.factor(fotocasa$cluster)
fotocasa$ownerType <- as.factor(fotocasa$ownerType)
fotocasa$propertySubtypeId <- as.factor(fotocasa$propertySubtypeId)
fotocasa$tieneCalefaccion = as.factor(fotocasa$tieneCalefaccion)
fotocasa$tieneTrastero = as.factor(fotocasa$tieneTrastero)
fotocasa$tieneAscensor = as.factor(fotocasa$tieneAscensor)
fotocasa$hotWater = as.factor(fotocasa$hotWater)
fotocasa$tieneAireAcondicionado = as.factor(fotocasa$tieneAireAcondicionado)
numeric_vars <- fotocasa %>%
dplyr::select_if(is.numeric) %>%
dplyr::select(-priceDropBinary) %>%
colnames()
t_pvals <- sapply(numeric_vars, function(var) {
t.test(fotocasa[[var]] ~ fotocasa$priceDropBinary)$p.value
})
low_sig_num <- names(t_pvals[t_pvals > 0.2])
# 2. Variables categóricas: chi-cuadrado o Fisher si es necesario
cat_vars <- fotocasa %>%
select_if(~ is.factor(.) | is.character(.)) %>%
colnames()
chi_pvals <- sapply(cat_vars, function(var) {
tbl <- table(fotocasa[[var]], fotocasa$priceDropBinary)
if (all(dim(tbl) > 1)) {
if (any(tbl < 5)) {
fisher.test(tbl, simulate.p.value = TRUE, B = 2000)$p.value
} else {
chisq.test(tbl)$p.value
}
} else {
1  # Si no hay variación
}
})
low_sig_cat <- names(chi_pvals[chi_pvals > 0.2])
# 3. Unir las variables irrelevantes
vars_to_remove <- union(low_sig_num, low_sig_cat)
# 4. Crear dataset reducido
fotocasa_significativo <- fotocasa[, !(names(fotocasa) %in% vars_to_remove)]
set.seed(456)
train_indices <- sample(1:nrow(fotocasa_significativo), size = 0.7 * nrow(fotocasa_significativo))
fotocasa_train <- fotocasa_significativo[train_indices, ]
fotocasa_test  <- fotocasa_significativo[-train_indices, ]
# --- Undersampling SOLO al entrenamiento ---
fotocasa_train_0 <- filter(fotocasa_train, priceDropBinary == 0)
fotocasa_train_1 <- filter(fotocasa_train, priceDropBinary == 1)
n1 <- nrow(fotocasa_train_1)
n0_target <- n1  # misma cantidad que clase 1
set.seed(456)
fotocasa_train_bal <- bind_rows(sample_n(fotocasa_train_0, n0_target), fotocasa_train_1)
# --- Combinar para model.matrix ---
fotocasa_train_bal$set <- "train"
fotocasa_test$set <- "test"
fotocasa_both <- bind_rows(fotocasa_train_bal, fotocasa_test)
# Variables como factor
fotocasa_both <- fotocasa_both %>%
mutate(
cluster = as.factor(cluster),
propertySubtypeId = as.factor(propertySubtypeId),
ownerType = as.factor(ownerType),
tieneCalefaccion = as.factor(tieneCalefaccion),
hotWater = as.factor(hotWater),
)
# Quitar las columnas priceDropBinary y set
X_data <- fotocasa_both %>%
dplyr::select(-priceDropBinary, -set)
# Crear matriz de diseño
X_all <- model.matrix(~ . - 1, data = X_data)
# Escalar
X_all <- scale(X_all)
# Separar nuevamente
X_train <- X_all[fotocasa_both$set == "train", ]
X_test  <- X_all[fotocasa_both$set == "test", ]
Y_train <- as.factor(fotocasa_train_bal$priceDropBinary)
Y_test  <- as.factor(fotocasa_test$priceDropBinary)
plsda_model <- plsda(X_train, Y_train, ncomp= 10)  # 10 componentes posibles
set.seed(456)
#perf_plsda <- perf(plsda_model, validation = "Mfold", folds = 5, nrepeat = 10)
# Media del BER por componente
#ber_means <- apply(perf_plsda$error.rate$BER, 2, mean)
#optimal_ncomp <- which.min(ber_means)
optimal_ncomp=1
cat("Componentes óptimos:", optimal_ncomp, "\n")
scores <- plsda_model$variates$X[, 1]
# Visualizar distribución por clase
df <- data.frame(Score = scores, Clase = Y_train)
ggplot(df, aes(x = Score, fill = Clase)) +
geom_density(alpha = 0.5) +
labs(title = "Distribución en la Componente 1", x = "Score (Comp 1)", y = "Densidad") +
theme_minimal()
library(pROC)
# 1. Predicción de clases
pred <- predict(plsda_model, newdata = X_test)
pred_class <- pred$class$max.dist[, optimal_ncomp]
# 2. Obtener probabilidades de la clase 1
prob_class1 <- pred$predict[, "1", optimal_ncomp]
# 3. Matriz de confusión
conf_matrix <- table(Predicho = factor(pred_class, levels = c(0,1)),
Real = factor(Y_test, levels = c(0,1)))
# 4. Extraer métricas
tp <- ifelse("1" %in% rownames(conf_matrix) && "1" %in% colnames(conf_matrix), conf_matrix["1", "1"], 0)
tn <- ifelse("0" %in% rownames(conf_matrix) && "0" %in% colnames(conf_matrix), conf_matrix["0", "0"], 0)
fp <- ifelse("1" %in% rownames(conf_matrix) && "0" %in% colnames(conf_matrix), conf_matrix["1", "0"], 0)
fn <- ifelse("0" %in% rownames(conf_matrix) && "1" %in% colnames(conf_matrix), conf_matrix["0", "1"], 0)
accuracy <- (tp + tn) / sum(conf_matrix)
recall <- ifelse((tp + fn) > 0, tp / (tp + fn), NA)  # Sensibilidad
specificity <- ifelse((tn + fp) > 0, tn / (tn + fp), NA)
balanced_accuracy <- mean(c(recall, specificity))
# 2. Curva ROC
roc_obj <- suppressMessages(roc(Y_test, prob_class1))
plot(roc_obj,
main = "Curva ROC",
col = "#2C3E50",
lwd = 2,
legacy.axes = TRUE)
# Filtrar umbrales válidos
thresholds <- roc_obj$thresholds
thresholds <- thresholds[thresholds >= 0 & thresholds <= 1]
# Calcular Balanced Accuracy en cada umbral
balanced_accs <- sapply(thresholds, function(t) {
pred <- ifelse(prob_class1 >= t, 1, 0)
conf <- table(factor(pred, levels = c(0,1)), Y_test)
tp <- ifelse("1" %in% rownames(conf) && "1" %in% colnames(conf), conf["1", "1"], 0)
tn <- ifelse("0" %in% rownames(conf) && "0" %in% colnames(conf), conf["0", "0"], 0)
fp <- ifelse("1" %in% rownames(conf) && "0" %in% colnames(conf), conf["1", "0"], 0)
fn <- ifelse("0" %in% rownames(conf) && "1" %in% colnames(conf), conf["0", "1"], 0)
sens <- if ((tp + fn) > 0) tp / (tp + fn) else NA
spec <- if ((tn + fp) > 0) tn / (tn + fp) else NA
mean(c(sens, spec), na.rm = TRUE)
})
# Elegir umbral óptimo
umbral_optimo <- thresholds[which.max(balanced_accs)]
cat("Umbral óptimo para máxima Balanced Accuracy:", round(umbral_optimo, 3), "\n")
# Aplicar predicción con el umbral óptimo
pred_optimo <- ifelse(prob_class1 >= umbral_optimo, 1, 0)
conf_matrix <- table(Predicho = pred_optimo, Real = Y_test)
print(conf_matrix)
# Métricas finales
tp <- conf_matrix["1", "1"]
tn <- conf_matrix["0", "0"]
fp <- conf_matrix["1", "0"]
fn <- conf_matrix["0", "1"]
accuracy <- (tp + tn) / sum(conf_matrix)
sens <- tp / (tp + fn)
spec <- tn / (tn + fp)
balanced_acc <- (sens + spec) / 2
cat("Accuracy:", round(accuracy, 3), "\n")
cat("Balanced Accuracy:", round(balanced_acc, 3), "\n")
library(forcats)# 1. Extraer las cargas (loadings) de cada componente
loadings_comp1 <- selectVar(plsda_model, comp = 1)$value
# 2. Crear dataframes con las contribuciones
df1 <- data.frame(
Variable = rownames(loadings_comp1),
Contribucion = loadings_comp1[, 1],
Componente = "Comp 1"
)
# 3. Seleccionar las 10 más influyentes por componente
top_n <- 10
df1_top <- df1[order(abs(df1$Contribucion), decreasing = TRUE)[1:top_n], ]
p1 <- ggplot(df1_top, aes(x = fct_reorder(Variable, Contribucion), y = Contribucion)) +
geom_col(fill = "#B28DFF") +
coord_flip() +
labs(title = "Contribución en Comp. 1", x = NULL, y = "Peso") +
theme_minimal(base_size = 9)
p1
fotocasa = read_excel("fotocasaImp.xlsx")
fotocasa <- as.data.frame(fotocasa)
rownames(fotocasa) <- as.character(1:nrow(fotocasa))
variables <- fotocasa[, c(9,10,11,13,14,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31,32,33)]
# Definir variables predictoras y variable objetivo
X <- variables[, !(names(variables) %in% c("priceAmount", "priceAmountDrop"))]  # Excluir "Precio"
Y <- variables$priceAmount  # Variable objetivo
invisible(
capture.output(
suppressMessages(
mypls <- opls(x = X, y = Y, predI = NA, crossvalI = 10,
scaleC = "standard", fig.pdfC = "none")
)
)
)
maxNC <- min(dim(X))
invisible(
capture.output(
suppressMessages(
myplsC <- opls(x = X, y = Y, predI = maxNC, crossvalI = 10,
scaleC = "standard", fig.pdfC = "none")
)
)
)
# Este chunk hay que ejecutarlo todo de una, paso a paso da error.
# mypls@modelDF  ## Para recuperar la información de cada componente
plot(1:maxNC, myplsC@modelDF$`R2Y(cum)`, type = "o", pch = 16, col = "blue3",
lwd = 2, xlab = "Components", ylab = "", ylim = c(0.4, 0.6),
main = "PLS model")
lines(1:maxNC, myplsC@modelDF$`Q2(cum)`, type = "o", pch = 16, col = "red3",
lwd = 2)
abline(h = 0.525, col = "red3", lty = 2)
legend("bottomleft", c("R2Y", "Q2"), lwd = 2,
col = c("blue3", "red3"), bty = "n")
A <- 4
invisible({
pdf(file = NULL)
mypls <- opls(x = X, y = Y, predI = A, crossvalI = 10, scaleC = "standard")
dev.off()
})
# Obtener pesos estandarizados (loadings) de X
pesos <- mypls@loadingMN
# Para cada componente, ordenamos las variables por su valor absoluto
top_vars_por_componente <- apply(pesos, 2, function(comp) {
orden <- order(abs(comp), decreasing = TRUE)
head(data.frame(Variable = rownames(pesos)[orden], Peso = comp[orden]), 5)
})
# Mostrar solo la tabla del primer componente
top_vars_por_componente[[1]]
par(mfrow = c(1, 2), mar = c(4, 4, 2, 1))  # Ajuste de márgenes
plot(mypls@scoreMN[,1], mypls@uMN[,1], xlab = "t", ylab = "u",
main = "Component 1", col = "red3")
abline(a = 0, b = 1, col = "grey", lty = 3)
plot(mypls@scoreMN[,2], mypls@uMN[,2], xlab = "t", ylab = "u",
main = "Component 2", col = "red3")
abline(a = 0, b = 1, col = "grey", lty = 3)
diag(cor(mypls@scoreMN, mypls@uMN))
# Función para calcular R2 global y por variable
R2 <- function(Y, myYpred){
SCT_k <- apply(scale(Y), 2, function(i) sum(i^2))      # Suma de cuadrados total por variable
SCE_k <- apply(myYpred, 2, function(i) sum(i^2))       # Suma de cuadrados explicada por variable
R2_k <- SCE_k / SCT_k                                  # R2 por variable
R2_total <- sum(SCE_k) / sum(SCT_k)                    # R2 global
list(R2_kcum = R2_k,
R2cum   = R2_total)
}
# Cálculo del R2 en el espacio de las X
myT <- mypls@scoreMN
myP <- mypls@loadingMN
myXpred <- myT %*% t(myP)  # Reconstrucción aproximada de X
R2X <- R2(X, myXpred)
# Mostrar top 10 variables con mayor R²
library(knitr)
r2_df <- data.frame(Variable = names(R2X$R2_kcum), R2 = R2X$R2_kcum)
r2_top <- head(r2_df[order(-r2_df$R2), ], 10)
kable(r2_top, digits = 2, caption = "Top 10 variables según R²")
# Predicciones ajustadas para Y desde el modelo PLS
myYpred <- fitted(mypls)
# Asegurar que tanto Y como myYpred sean matrices
Y_mat <- as.matrix(Y)
myYpred_mat <- as.matrix(myYpred)
# Función R2 definida previamente
R2 <- function(Y, myYpred) {
# Asegura que ambos estén escalados igual
Y <- as.matrix(Y)
myYpred <- as.matrix(myYpred)
# Suma de cuadrados total por variable (desviación respecto a la media)
SCT_k <- apply(Y, 2, function(i) sum((i - mean(i))^2))
# Suma de cuadrados del error (diferencia entre real y predicho)
SCR_k <- apply((Y - myYpred)^2, 2, sum)
# R^2 por variable
R2_k <- 1 - SCR_k / SCT_k
# R^2 total
R2_total <- 1 - sum(SCR_k) / sum(SCT_k)
list(R2_kcum = R2_k,
R2cum   = R2_total)
}
# Calcular R2 para Y
R2Y <- R2(Y, myYpred)
# Mostrar resultados
R2Y
calcular_errores <- function(y_real, y_pred) {
y_real <- as.numeric(y_real)
y_pred <- as.numeric(y_pred)
# R²
ss_res <- sum((y_real - y_pred)^2)
ss_tot <- sum((y_real - mean(y_real))^2)
R2 <- 1 - ss_res / ss_tot
# RMSE
RMSE <- sqrt(mean((y_real - y_pred)^2))
# MAE
MAE <- mean(abs(y_real - y_pred))
# MAPE
MAPE <- mean(abs((y_real - y_pred) / y_real)) * 100
return(data.frame(R2 = R2, RMSE = RMSE, MAE = MAE, MAPE = MAPE))
}
# Asegura que Y es numérico
y_real <- as.numeric(Y)
# Predicciones del modelo completo
y_pred_completo <- fitted(myplsC)
# Calcular métricas
errores_completo <- calcular_errores(y_real, y_pred_completo)
# Mostrar resultados
print(round(errores_completo, 4))
knitr::include_graphics("mapa_clusters.png")
include_graphics("mapa_clusters.png")
