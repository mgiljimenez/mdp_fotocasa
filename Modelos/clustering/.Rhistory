midist_eu <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist_eu, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MANHATTAN
midist_manhattan <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
fviz_dist(midist_manhattan, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MAXIMA
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "maximum")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
library(readxl)
fotocasa = read_excel("fotocasaImp.xlsx")
fotocasa <- as.data.frame(fotocasa)
rownames(fotocasa) <- fotocasa[[1]]
fotocasa
variables <- fotocasa[, c(9,10,11,13,14,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31, 32, 33)]
library(knitr)
library(FactoMineR)
library(factoextra)
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = 10, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:7,])
# Umbral 99 %
atipicos_moderados=fotocasa$URL[ mySCR > chi2lim99 ]
myE = X - misScores %*% t(misLoadings)
K=3
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = K, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:K,])
K=3
res.pca_reducido = PCA(variables_reducido, scale.unit = TRUE, graph = FALSE, ncp = K, quanti.sup=16)
library(readxl)
fotocasa = read_excel("fotocasaImp.xlsx")
fotocasa <- as.data.frame(fotocasa)
rownames(fotocasa) <- fotocasa[[1]]
fotocasa
variables <- fotocasa[, c(9,10,11,13,14,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31, 32, 33)]
library(knitr)
library(FactoMineR)
library(factoextra)
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = 10, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:7,])
#install.packages("caret")  # Si no lo tienes instalado
#install.packages("pls")    # Para PCA + Regresión
library(caret)
library(pls)
set.seed(123)  # Fijamos semilla para reproducibilidad
# Definir variables predictoras y variable objetivo
X <- variables[, !(names(variables) %in% c("priceAmount", "priceAmountDrop"))]  # Excluir "Precio"
Y <- variables$priceAmount  # Variable objetivo
# Configuración de validación cruzada (10-fold cross-validation)
control <- trainControl(method = "cv", number = 10)
# Entrenar modelo de regresión con PCA
modelo_pcr <- train(
priceAmount ~ .,      # Fórmula: Predecir Precio con todas las variables
data = variables[,c(1:22)],
method = "pcr",  # "pcr" = Regresión con PCA
trControl = control,
tuneLength = 10  # Probar hasta 10 componentes principales
)
# Mostrar resumen del modelo
print(modelo_pcr)
# Graficar error vs. número de componentes principales
plot(modelo_pcr)
K=3
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = K, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:K,])
misScores = res.pca$ind$coord[,1:K]
miT2 = colSums(t(misScores**2)/eig.val[1:K,1])
I = nrow(variables)
F95 = K*(I**2 - 1)/(I*(I - K)) * qf(0.95, K, I-K)
F99 = K*(I**2 - 1)/(I*(I - K)) * qf(0.99, K, I-K)
plot(1:length(miT2), miT2, type = "p", xlab = "Pisos", ylab = "T2")
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)
library(grid)
library(gridExtra)
p1 = fviz_pca_ind(res.pca, axes = c(1,2), geom = c("point"),
habillage = factor(miT2 > F95)) +
tune::coord_obs_pred()
p2 = fviz_pca_ind(res.pca, axes = c(1,3), geom = c("point"),
habillage = factor(miT2 > F95)) +
tune::coord_obs_pred()
grid.arrange(p1,p2, nrow = 1)
contribT2 = function (X, scores, loadings, eigenval, observ, cutoff = 2) {
# X is data matrix and must be centered (or centered and scaled if data were scaled)
misScoresNorm = t(t(scores**2) / eigenval)
misContrib = NULL
for (oo in observ) {
print(rownames(scores)[oo])
print(scores[oo,])
misPCs = which(as.numeric(misScoresNorm[oo,]) > cutoff)
lacontri = sapply(misPCs, function (cc) (scores[oo,cc]/eigenval[cc])*loadings[,cc]*X[oo,])
lacontri = rowSums((1*(sign(lacontri) == 1))*lacontri)
misContrib = cbind(misContrib, lacontri)
}
colnames(misContrib) = rownames(misScoresNorm[observ,])
return(misContrib)
}
# Recuperamos los datos utilizados en el modelo PCA, centrados y escalados
variablesCE = variables[,setdiff(colnames(variables), c("priceAmountDrop"))]
variablesCE = scale(variablesCE, center = TRUE, scale = TRUE)
X = as.matrix(variablesCE)
# Calculamos los loadings a partir de las coordenadas de las variables
# ya que la librería FactoMineR nos devuelve los loadings ponderados
# por la importancia de cada componente principal.
misLoadings = sweep(res.pca$var$coord, 2, sqrt(res.pca$eig[1:K,1]), FUN="/")
# Calculamos las contribuciones
mycontrisT2 = contribT2(X = X, scores = misScores, loadings = misLoadings,
eigenval = eig.val[1:K,1], observ = which.max(miT2),
cutoff = 2)
par(mar = c(10,2.3,3,1))
barplot(mycontrisT2[,1],las=2, #cex.names = 0.5,
main= paste0("Observación: ", rownames(variables)[which.max(miT2)]))
# Suponiendo que tienes tus scores, loadings y la cantidad de componentes que deseas usar (3 en este caso)
predicted_price = sum(scores[which.max(miT2), 1:3] * misLoadings[1:3, 1:3])
par(mar = c(10,2.3,3,1))
barplot(mycontrisT2[,1],las=2, #cex.names = 0.5,
main= paste0("Observación: ", rownames(variables)[which.max(miT2)]))
# Suponiendo que tienes tus scores, loadings y la cantidad de componentes que deseas usar (3 en este caso)
predicted_price = sum(scores[which.max(miT2), 1:3] * misLoadings[1:3, 1:3])
real_price = variables$priceAmount[which.max(miT2)]  # Aquí usas el índice correspondiente
comparison = data.frame(Real_Price = real_price, Predicted_Price = predicted_price)
par(mar = c(10,2.3,3,1))
barplot(mycontrisT2[,1],las=2, #cex.names = 0.5,
main= paste0("Observación: ", rownames(variables)[which.max(miT2)]))
# Suponiendo que tienes tus scores, loadings y la cantidad de componentes que deseas usar (3 en este caso)
predicted_price = sum(scores[which.max(miT2), 1:3] * misLoadings[1:3, 1:3])
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tabla <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tabla)
fotocasaConfort = fotocasa[,tabla$Confort == 1]
fotocasaConfort = scale(fotocasaConfort, center = TRUE, scale = TRUE)
#distancia EUCLIDEA
midist_eu <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist_eu, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MANHATTAN
midist_manhattan <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
fviz_dist(midist_manhattan, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
library(grid)
library(gridExtra)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "silhouette",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "wss",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
clustWARD <- hclust(midist_manhattan, method="ward.D2")
gruposWARD <- cutree(clustWARD, k=7)
table(gruposWARD)
clustPAM <- pam(fotocasaConfort, k = 7)
table(clustPAM$clustering)
clustMEANS <- kmeans(fotocasaConfort, centers = 7, nstart = 20)
table(clustMEANS$cluster)
library(ggsci)
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(gruposWARD, midist_manhattan), col=colores, border=NA, main = "WARD")
plot(silhouette(clustPAM$clustering, midist_manhattan), col=colores, border=NA, main = "K-MEDIOIDES")
plot(silhouette(clustMEANS$cluster, midist_eu), col=colores2, border=NA, main = "K-MEDIAS")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tabla <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tabla)
fotocasaConfort = fotocasa[,tabla$Confort == 1]
fotocasaConfort = scale(fotocasaConfort, center = TRUE, scale = TRUE)
#distancia EUCLIDEA
midist_eu <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist_eu, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
library(grid)
library(gridExtra)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "silhouette",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "wss",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
clustWARD <- hclust(midist_manhattan, method="ward.D2")
gruposWARD <- cutree(clustWARD, k=7)
table(gruposWARD)
clustPAM <- pam(fotocasaConfort, k = 7)
table(clustPAM$clustering)
clustMEANS <- kmeans(fotocasaConfort, centers = 7, nstart = 20)
table(clustMEANS$cluster)
library(ggsci)
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(gruposWARD, midist_manhattan), col=colores, border=NA, main = "WARD")
plot(silhouette(clustPAM$clustering, midist_manhattan), col=colores, border=NA, main = "K-MEDIOIDES")
plot(silhouette(clustMEANS$cluster, midist_eu), col=colores2, border=NA, main = "K-MEDIAS")
=======
# Añadir el cluster a tu dataframe original
library(ggplot2)
library(leaflet)
ggplot() +
geom_point(aes(x = fotocasa$lng,
y = fotocasa$lat,
color = factor(fotocasaServicios$cluster)),
size = 2, alpha = 0.7) +
labs(title = "Clustering geográfico de inmuebles",
x = "Longitud", y = "Latitud", color = "Cluster") +
theme_minimal()
fotocasaServicios$cluster=clustPAM$clustering
# Añadir el cluster a tu dataframe original
library(ggplot2)
library(leaflet)
ggplot() +
geom_point(aes(x = fotocasa$lng,
y = fotocasa$lat,
color = factor(fotocasaServicios$cluster)),
size = 2, alpha = 0.7) +
labs(title = "Clustering geográfico de inmuebles",
x = "Longitud", y = "Latitud", color = "Cluster") +
theme_minimal()
fotocasaServicios$cluster=gruposWARD
# Añadir el cluster a tu dataframe original
library(ggplot2)
library(leaflet)
ggplot() +
geom_point(aes(x = fotocasa$lng,
y = fotocasa$lat,
color = factor(fotocasaServicios$cluster)),
size = 2, alpha = 0.7) +
labs(title = "Clustering geográfico de inmuebles",
x = "Longitud", y = "Latitud", color = "Cluster") +
theme_minimal()
fotocasaServicios$cluster=clustMEANS$cluster
# Añadir el cluster a tu dataframe original
library(ggplot2)
library(leaflet)
ggplot() +
geom_point(aes(x = fotocasa$lng,
y = fotocasa$lat,
color = factor(fotocasaServicios$cluster)),
size = 2, alpha = 0.7) +
labs(title = "Clustering geográfico de inmuebles",
x = "Longitud", y = "Latitud", color = "Cluster") +
theme_minimal()
install.packages(c("ggmap", "viridis"))
library(viridis)
library(ggmap)
#install.packages(c("ggmap", "viridis"))
library(ggmap)
library(viridis)
# Obtener mapa base de Stamen (gratis, no requiere API)
valencia_map <- get_map(location = "Valencia, Spain", zoom = 13, source = "stamen", maptype = "toner-lite")
#install.packages(c("ggmap", "viridis"))
library(ggmap)
library(viridis)
# Obtener mapa base de Stamen (gratis, no requiere API)
valencia_map <- get_map(location = "Valencia, Spain", zoom = 13, source = "google", maptype = "toner-lite")
#install.packages(c("ggmap", "viridis"))
library(ggmap)
library(viridis)
# Obtener mapa base de Stamen (gratis, no requiere API)
valencia_map <- get_stamenmap(
bbox = c(left = -0.42, bottom = 39.44, right = -0.30, top = 39.52),
zoom = 13,
maptype = "toner-lite"
)
#install.packages(c("ggmap", "viridis"))
library(ggmap)
library(viridis)
# Obtener mapa base de Stamen (gratis, no requiere API)
valencia_map <- get_stadiamap(
bbox = c(left = -0.42, bottom = 39.44, right = -0.30, top = 39.52),
zoom = 13,
maptype = "toner-lite"
)
#install.packages(c("ggmap", "viridis"))
library(ggmap)
library(viridis)
# Obtener mapa base de Stamen (gratis, no requiere API)
valencia_map <- get_stadiamap(arg="stamen_toner_lite",
bbox = c(left = -0.42, bottom = 39.44, right = -0.30, top = 39.52),
zoom = 13,
maptype = "toner-lite"
)
#install.packages(c("ggmap", "viridis"))
library(ggmap)
library(viridis)
# Obtener mapa base de Stamen (gratis, no requiere API)
valencia_map <- get_stadiamap(
bbox = c(left = -0.42, bottom = 39.44, right = -0.30, top = 39.52),
zoom = 13,
maptype = "stamen_toner_lite"
)
help("get_googlemap")
(map <- get_googlemap(c(-97.14667, 31.5493)))
help(get_map)
help(get_map)
map <- get_map(location = "texas", zoom = 6, source = "stadia")
map <- get_stadiamap(location = "texas", zoom = 6, source = "stadia")
library(leaflet)
library(RColorBrewer)
# Crear paleta con colores diferenciables
pal <- colorFactor(palette = "Set1", domain = fotocasaServicios$cluster)
leaflet() %>%
addTiles() %>%
addCircleMarkers(lng = fotocasa$lng,
lat = fotocasa$lat,
color = ~pal(fotocasaServicios$cluster),
radius = 5,
stroke = FALSE,
fillOpacity = 0.8) %>%
addLegend("bottomright", pal = pal, values = fotocasaServicios$cluster,
title = "Clusters") %>%
setView(lng = mean(fotocasa$lng),
lat = mean(fotocasa$lat),
zoom = 13)
library(leaflet)
leaflet() %>%
addTiles() %>%
setView(lng = -0.379, lat = 39.467, zoom = 13)  # Coordenadas aproximadas de Valencia
length(fotocasaServicios$cluster)
nrow(fotocasa)
head(fotocasa)
sum(is.na(fotocasa$lat))  # Cuántos NA hay en latitud
sum(is.na(fotocasa$lng))  # Cuántos NA hay en longitud
library(leaflet)
library(RColorBrewer)
# Crear paleta con colores diferenciados
pal <- colorFactor(palette = "Set1", domain = fotocasaServicios$cluster)
# Verificamos que 'df_coords' tenga las columnas adecuadas
if("lat" %in% colnames(fotocasa) && "lng" %in% colnames(fotocasa)) {
leaflet(fotocasa) %>%
addTiles() %>%
addCircleMarkers(
lng = ~lng,
lat = ~lat,
color = ~pal(fotocasaServicios$cluster),
radius = 5,
stroke = FALSE,
fillOpacity = 0.8
) %>%
addLegend("bottomright", pal = pal, values = fotocasaServicios$cluster,
title = "Clusters") %>%
setView(lng = mean(fotocasa$lng),
lat = mean(fotocasa$lat),
zoom = 13)
} else {
print("Faltan las columnas 'latitud' o 'longitud' en el dataframe.")
}
<<<<<<< HEAD
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
fotoCasa
library(readxl)
fotocasa = read_excel("fotocasaImp.xlsx")
colnames(fotocasa)
fotocasa
# Seleccionar la columna específica (reemplaza 'columna' con el nombre de tu columna)
drop <- fotocasa$priceAmountDrop
# Contar cuántas celdas son distintas de cero
distintos_de_cero <- sum(drop != 0)
# Calcular el porcentaje
porcentaje_distintos_de_cero <- (distintos_de_cero / length(drop)) * 100
# Mostrar el resultado
cat("El porcentaje de celdas distintas de cero en la columna es:", round(porcentaje_distintos_de_cero, 2), "%\n")
=======
>>>>>>> 626ffd02ac0c205d68f91fb349ad71e2bdd0b288
>>>>>>> e8fac913e6e195221fc88040eae74abe57439dde
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
library(readxl)
fotocasa = read_excel("fotocasaImp.xlsx")
colnames(fotocasa)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c("tieneAscensor", "tieneTrastero", "tieneCalefacción", "tieneAireAcondicionado", "bathrooms", "surface", "rooms"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tabla <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tabla)
fotocasaConfort = fotocasa[,tabla$Confort == 1]
head(fotocasaConfort)
fotocasaConfortCE = scale(fotocasaConfort, center = TRUE, scale = TRUE)
help(get_dist)
#distancia GOWER
midist_gower <- gower_dist <- daisy(fotocasaConfortCE, metric = "gower")
fviz_dist(midist_gower, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
library(grid)
library(gridExtra)
help("fviz_nbclust")
p1 = fviz_nbclust(x = fotocasaConfortCE, FUNcluster = hcut, diss=midist_gower,
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = fotocasaConfortCE, FUNcluster = hcut, method = "wss",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,diss=midist_gower,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
library(grid)
library(gridExtra)
help("fviz_nbclust")
p1 = fviz_nbclust(x = fotocasaConfortCE, FUNcluster = hcut, diss=midist_gower,
hc_method = "ward.D2", k.max = 10, verbose = FALSE) + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = fotocasaConfortCE, FUNcluster = hcut, method = "wss",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,diss=midist_gower) + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
