knitr::opts_chunk$set(echo = TRUE)
colnames(fotocasa)
library(readxl)
fotocasa = read_excel("fotocasaImp.xlsx")
colnames(fotocasa)
# Vector con nombres de todas las columnas
vars <- colnames(fotocasa)
# Definir las variables de confort
confort_vars <- c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios_vars <- grep("_count$", vars, value = TRUE)
# Crear el data.frame de flags
df_flags <- data.frame(
Variable   = vars,
Confort    = as.integer(vars %in% confort_vars),
Servicios  = as.integer(vars %in% servicios_vars),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(df_flags)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tablaVar <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tablaVar)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tabla <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tabla)
fotocasaConfort = fotocasaImp[,tabla$Confort == 1]
fotocasaConfort = fotocasa[,tabla$Confort == 1]
fotocasaConfort = scale(fotocasaConfort, center = TRUE, scale = TRUE)
fotocasaConfort
fotocasaConfort = fotocasa[,tabla$Confort == 1]
fotocasaConfort = scale(fotocasaConfort, center = TRUE, scale = TRUE)
midist <- get_dist(cereals2, stand = FALSE, method = "euclidean")
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
help("get_dist")
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
'''
#distancia EUCLIDEA
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MANHATTAN
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
'''
#distancia EUCLIDEA
#midist <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
#fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
#   gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MANHATTAN
#midist <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
#fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
#        gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MAXIMA
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "maximum")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MAXIMA
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "maximum")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia EUCLIDEA
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia CANBERRA
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "canberra")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
set.seed(100)
myN = c(20, 35, 50, 65)  # m
myhopkins = NULL
myseed = sample(1:1000, 10)
for (i in myN) {
for (j in myseed) {
tmp = get_clust_tendency(data = fotocasaConfort, n = i, graph = FALSE, seed = j)
myhopkins = c(myhopkins, tmp$hopkins_stat)
}
}
library(grid)
library(gridExtra)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "silhouette",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "wss",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
#distancia MANHATTAN
midist_manhattan <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
fviz_dist(midist_manhattan, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
clust1 <- hclust(midist_manhattan, method="ward.D2")
grupos1 <- cutree(clust1, k=7)
table(grupos1)
clust1 <- hclust(midist_manhattan, method="ward.D2")
grupos1 <- cutree(clust1, k=7)
table(grupos1)
fviz_dend(clust1, k = 7,
cex = 0.5, color_labels_by_k = TRUE,
rect = TRUE) # dibujar rectángulos
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
clustPAM <- pam(cereals2, k = 7)
clustPAM <- pam(fotocasaConfort, k = 7)
table(clustPAM$clustering)
clustWARD <- hclust(midist_manhattan, method="ward.D2")
gruposWARD <- cutree(clustWARD, k=7)
table(gruposWARD)
clustPAM <- pam(fotocasaConfort, k = 7)
table(clustPAM$clustering)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
clustWARD <- hclust(midist_manhattan, method="ward.D2")
gruposWARD <- cutree(clustWARD, k=7)
table(gruposWARD)
clustPAM <- pam(fotocasaConfort, k = 7)
table(clustPAM$clustering)
clustMEANS <- kmeans(fotocasaConfort, centers = 7, nstart = 20)
table(clustMEANS$cluster)
#distancia EUCLIDEA
midist_eu <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
library(ggsci)
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(gruposWARD, midist_manhattan), col=colores, border=NA, main = "WARD")
plot(silhouette(clustPAM$cluster, midist_manhattan), col=colores, border=NA, main = "K-MEDIAS")
plot(silhouette(clustMEANS$clustering, midist_eu), col=colores2, border=NA, main = "K-MEDOIDES")
library(ggsci)
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(gruposWARD, midist_manhattan), col=colores, border=NA, main = "WARD")
plot(silhouette(clustPAM$cluster, midist_manhattan), col=colores, border=NA, main = "K-MEDIOIDES")
plot(silhouette(clustMEANS$clustering, midist_eu), col=colores2, border=NA, main = "K-MEDIAS")
library(ggsci)
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(gruposWARD, midist_manhattan), col=colores, border=NA, main = "WARD")
plot(silhouette(clustPAM$clustering, midist_manhattan), col=colores, border=NA, main = "K-MEDIOIDES")
plot(silhouette(clustMEANS$cluster, midist_eu), col=colores2, border=NA, main = "K-MEDIAS")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tabla <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tabla)
fotocasaConfort = fotocasa[,tabla$Confort == 1]
fotocasaConfort = scale(fotocasaConfort, center = TRUE, scale = TRUE)
#distancia EUCLIDEA
midist_eu <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist_eu, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MANHATTAN
midist_manhattan <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
fviz_dist(midist_manhattan, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MAXIMA
midist <- get_dist(fotocasaConfort, stand = FALSE, method = "maximum")
fviz_dist(midist, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
library(readxl)
fotocasa = read_excel("fotocasaImp.xlsx")
fotocasa <- as.data.frame(fotocasa)
rownames(fotocasa) <- fotocasa[[1]]
fotocasa
variables <- fotocasa[, c(9,10,11,13,14,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31, 32, 33)]
library(knitr)
library(FactoMineR)
library(factoextra)
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = 10, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:7,])
# Umbral 99 %
atipicos_moderados=fotocasa$URL[ mySCR > chi2lim99 ]
myE = X - misScores %*% t(misLoadings)
K=3
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = K, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:K,])
K=3
res.pca_reducido = PCA(variables_reducido, scale.unit = TRUE, graph = FALSE, ncp = K, quanti.sup=16)
library(readxl)
fotocasa = read_excel("fotocasaImp.xlsx")
fotocasa <- as.data.frame(fotocasa)
rownames(fotocasa) <- fotocasa[[1]]
fotocasa
variables <- fotocasa[, c(9,10,11,13,14,15,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31, 32, 33)]
library(knitr)
library(FactoMineR)
library(factoextra)
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = 10, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:7,])
#install.packages("caret")  # Si no lo tienes instalado
#install.packages("pls")    # Para PCA + Regresión
library(caret)
library(pls)
set.seed(123)  # Fijamos semilla para reproducibilidad
# Definir variables predictoras y variable objetivo
X <- variables[, !(names(variables) %in% c("priceAmount", "priceAmountDrop"))]  # Excluir "Precio"
Y <- variables$priceAmount  # Variable objetivo
# Configuración de validación cruzada (10-fold cross-validation)
control <- trainControl(method = "cv", number = 10)
# Entrenar modelo de regresión con PCA
modelo_pcr <- train(
priceAmount ~ .,      # Fórmula: Predecir Precio con todas las variables
data = variables[,c(1:22)],
method = "pcr",  # "pcr" = Regresión con PCA
trControl = control,
tuneLength = 10  # Probar hasta 10 componentes principales
)
# Mostrar resumen del modelo
print(modelo_pcr)
# Graficar error vs. número de componentes principales
plot(modelo_pcr)
K=3
res.pca = PCA(variables, scale.unit = TRUE, graph = FALSE, ncp = K, quanti.sup=23)
eig.val <- get_eigenvalue(res.pca)
VPmedio = 100 * (1/nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept=VPmedio, linetype=2, color="red")
kable(eig.val[1:K,])
misScores = res.pca$ind$coord[,1:K]
miT2 = colSums(t(misScores**2)/eig.val[1:K,1])
I = nrow(variables)
F95 = K*(I**2 - 1)/(I*(I - K)) * qf(0.95, K, I-K)
F99 = K*(I**2 - 1)/(I*(I - K)) * qf(0.99, K, I-K)
plot(1:length(miT2), miT2, type = "p", xlab = "Pisos", ylab = "T2")
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)
library(grid)
library(gridExtra)
p1 = fviz_pca_ind(res.pca, axes = c(1,2), geom = c("point"),
habillage = factor(miT2 > F95)) +
tune::coord_obs_pred()
p2 = fviz_pca_ind(res.pca, axes = c(1,3), geom = c("point"),
habillage = factor(miT2 > F95)) +
tune::coord_obs_pred()
grid.arrange(p1,p2, nrow = 1)
contribT2 = function (X, scores, loadings, eigenval, observ, cutoff = 2) {
# X is data matrix and must be centered (or centered and scaled if data were scaled)
misScoresNorm = t(t(scores**2) / eigenval)
misContrib = NULL
for (oo in observ) {
print(rownames(scores)[oo])
print(scores[oo,])
misPCs = which(as.numeric(misScoresNorm[oo,]) > cutoff)
lacontri = sapply(misPCs, function (cc) (scores[oo,cc]/eigenval[cc])*loadings[,cc]*X[oo,])
lacontri = rowSums((1*(sign(lacontri) == 1))*lacontri)
misContrib = cbind(misContrib, lacontri)
}
colnames(misContrib) = rownames(misScoresNorm[observ,])
return(misContrib)
}
# Recuperamos los datos utilizados en el modelo PCA, centrados y escalados
variablesCE = variables[,setdiff(colnames(variables), c("priceAmountDrop"))]
variablesCE = scale(variablesCE, center = TRUE, scale = TRUE)
X = as.matrix(variablesCE)
# Calculamos los loadings a partir de las coordenadas de las variables
# ya que la librería FactoMineR nos devuelve los loadings ponderados
# por la importancia de cada componente principal.
misLoadings = sweep(res.pca$var$coord, 2, sqrt(res.pca$eig[1:K,1]), FUN="/")
# Calculamos las contribuciones
mycontrisT2 = contribT2(X = X, scores = misScores, loadings = misLoadings,
eigenval = eig.val[1:K,1], observ = which.max(miT2),
cutoff = 2)
par(mar = c(10,2.3,3,1))
barplot(mycontrisT2[,1],las=2, #cex.names = 0.5,
main= paste0("Observación: ", rownames(variables)[which.max(miT2)]))
# Suponiendo que tienes tus scores, loadings y la cantidad de componentes que deseas usar (3 en este caso)
predicted_price = sum(scores[which.max(miT2), 1:3] * misLoadings[1:3, 1:3])
par(mar = c(10,2.3,3,1))
barplot(mycontrisT2[,1],las=2, #cex.names = 0.5,
main= paste0("Observación: ", rownames(variables)[which.max(miT2)]))
# Suponiendo que tienes tus scores, loadings y la cantidad de componentes que deseas usar (3 en este caso)
predicted_price = sum(scores[which.max(miT2), 1:3] * misLoadings[1:3, 1:3])
real_price = variables$priceAmount[which.max(miT2)]  # Aquí usas el índice correspondiente
comparison = data.frame(Real_Price = real_price, Predicted_Price = predicted_price)
par(mar = c(10,2.3,3,1))
barplot(mycontrisT2[,1],las=2, #cex.names = 0.5,
main= paste0("Observación: ", rownames(variables)[which.max(miT2)]))
# Suponiendo que tienes tus scores, loadings y la cantidad de componentes que deseas usar (3 en este caso)
predicted_price = sum(scores[which.max(miT2), 1:3] * misLoadings[1:3, 1:3])
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tabla <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tabla)
fotocasaConfort = fotocasa[,tabla$Confort == 1]
fotocasaConfort = scale(fotocasaConfort, center = TRUE, scale = TRUE)
#distancia EUCLIDEA
midist_eu <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist_eu, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
#distancia MANHATTAN
midist_manhattan <- get_dist(fotocasaConfort, stand = FALSE, method = "manhattan")
fviz_dist(midist_manhattan, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
library(grid)
library(gridExtra)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "silhouette",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "wss",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
clustWARD <- hclust(midist_manhattan, method="ward.D2")
gruposWARD <- cutree(clustWARD, k=7)
table(gruposWARD)
clustPAM <- pam(fotocasaConfort, k = 7)
table(clustPAM$clustering)
clustMEANS <- kmeans(fotocasaConfort, centers = 7, nstart = 20)
table(clustMEANS$cluster)
library(ggsci)
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(gruposWARD, midist_manhattan), col=colores, border=NA, main = "WARD")
plot(silhouette(clustPAM$clustering, midist_manhattan), col=colores, border=NA, main = "K-MEDIOIDES")
plot(silhouette(clustMEANS$cluster, midist_eu), col=colores2, border=NA, main = "K-MEDIAS")
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(cluster)
library(FactoMineR)
library(factoextra)
library(NbClust)
library(clValid)
# Vector con nombres de todas las columnas
columnas= colnames(fotocasa)
# Definir las variables de confort
confort = c(
"bathrooms", "floor", "hotWater", "rooms", "surface",
"tieneAscensor", "tieneTrastero", "tieneCalefaccion", "tieneAireAcondicionado"
)
# Identificar automáticamente las variables de servicios (aquellas que terminan en "_count")
servicios = grep("_count$", columnas, value = TRUE)
# Crear el data.frame de flags
tabla <- data.frame(
Variable   = columnas,
Confort    = as.integer(columnas %in% confort),
Servicios  = as.integer(columnas %in% servicios),
stringsAsFactors = FALSE
)
# Mostrar resultado
print(tabla)
fotocasaConfort = fotocasa[,tabla$Confort == 1]
fotocasaConfort = scale(fotocasaConfort, center = TRUE, scale = TRUE)
#distancia EUCLIDEA
midist_eu <- get_dist(fotocasaConfort, stand = FALSE, method = "euclidean")
fviz_dist(midist_eu, show_labels = TRUE, lab_size = 0.3,
gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"))
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
library(grid)
library(gridExtra)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "silhouette",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = hcut, method = "wss",
hc_method = "ward.D2", k.max = 10, verbose = FALSE,
hc_metric = "manhattan") + labs(title = "Num. optimo clusters")
grid.arrange(p1, p2, nrow = 1)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = pam, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "Numero optimo de clusters")
grid.arrange(p1, p2, nrow = 1)
p1 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "silhouette",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
p2 = fviz_nbclust(x = fotocasaConfort, FUNcluster = kmeans, method = "wss",
k.max = 10, verbose = FALSE) +
labs(title = "K-means")
grid.arrange(p1, p2, nrow = 1)
clustWARD <- hclust(midist_manhattan, method="ward.D2")
gruposWARD <- cutree(clustWARD, k=7)
table(gruposWARD)
clustPAM <- pam(fotocasaConfort, k = 7)
table(clustPAM$clustering)
clustMEANS <- kmeans(fotocasaConfort, centers = 7, nstart = 20)
table(clustMEANS$cluster)
library(ggsci)
colores = pal_npg("nrc")(6)
colores2 = pal_npg("nrc")(7)
par(mfrow = c(1,3))
plot(silhouette(gruposWARD, midist_manhattan), col=colores, border=NA, main = "WARD")
plot(silhouette(clustPAM$clustering, midist_manhattan), col=colores, border=NA, main = "K-MEDIOIDES")
plot(silhouette(clustMEANS$cluster, midist_eu), col=colores2, border=NA, main = "K-MEDIAS")
