

```{r}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(caret)
# Librer√≠a para validaci√≥n de m√©todos supervisados
# https://cran.r-project.org/web/packages/caret/vignettes/caret.html
# http://topepo.github.io/caret/index.html
library(MASS) # Para el an√°lisis discriminante
library(readxl)
library(fastDummies)

library(dplyr)
```

```{r datos}
fotocasa = read_excel("fotocasaImp.xlsx")
# Crear la nueva columna binaryDrop
fotocasa$binaryDrop <- ifelse(fotocasa$priceAmountDrop > 0, 1, 0)


# Mostrar el dataframe resultante
colnames(fotocasa)
```


En este analisis, intentaremos aplicar la funcion lineal de fisher para predecir, en funci√≥n de sus caracter√≠sticas, qu√© inmuebles bajar√°n de precio. Lo haremos creando binaryDrop a partir de la variable num√©rica priceAmountDrop.Los valores de esta nueva variable ser√°n 0 si priceAmountDrop=0 y 1 en el caso contrario (el precio ha bajado recientemente). Consideraremos positivo binaryDrop=1.
Puesto que las clases se encuentran severamente desbalanceadas (√∫nicamente un 14% de la muestra han bajado de precio), aplicaremos un undersampling para conseguir una proporci√≥n 2:1, qued√°ndonos con todos los positivos y parte de los negativos.

```{r selecci√≥n_variables}

# Convertir variables categ√≥ricas a factor
data <- fotocasa %>%
  mutate(
    binaryDrop = as.factor(binaryDrop),
    neighborhood = as.factor(neighborhood),
    tieneAscensor = as.factor(tieneAscensor),
    tieneTrastero = as.factor(tieneTrastero),
    tieneCalefaccion = as.factor(tieneCalefaccion),
    tieneAireAcondicionado = as.factor(tieneAireAcondicionado),
    propertySubtypeId=as.factor(propertySubtypeId)
  )

# Seleccionar variables relevantes
fotocasa <- data %>%
  select(binaryDrop,
         surface,
         rooms,
         bathrooms,
         floor,
         tieneAscensor,
         tieneTrastero,
         tieneCalefaccion,
         tieneAireAcondicionado,
         neighborhood,  # puedes cambiar por municipality si lo prefieres
         priceAmount,
         priceDescription_buy,
         priceDescription_rent,
         propertySubtypeId,
         hospital_count,
         supermarket_count,
         pharmacy_count,
         university_count,
         college_count,
         public_transport_count
         
         )
head(fotocasa)
```
```{r dummies}
library(caret)

# Especifica tus columnas categ√≥ricas nominales
categoricas <- c("propertySubtypeId", "neighborhood")
fotocasa <- dummy_cols(fotocasa,
                       select_columns = categoricas,
                       remove_first_dummy = TRUE,
                       remove_selected_columns = TRUE)

# 3. Detectar columnas dummy reci√©n creadas (basado en nombres)
cols_dummies <- grep(paste0("^(", paste(categoricas, collapse = "|"), ")_"), names(fotocasa), value = TRUE)

# 4. Convertir esas columnas de 0/1 a factores con niveles "0", "1"
fotocasa[cols_dummies] <- lapply(fotocasa[cols_dummies], function(x) factor(x, levels = c(0, 1)))
```


```{r}
# Transformaciones logar√≠tmicas para corregir asimetr√≠a positiva (sobrescribiendo variables)
fotocasa$priceDescription_rent <- log1p(fotocasa$priceDescription_rent)
fotocasa$priceDescription_buy <- log1p(fotocasa$priceDescription_buy)
fotocasa$supermarket_count <- log1p(fotocasa$supermarket_count)
fotocasa$pharmacy_count <- log1p(fotocasa$pharmacy_count)
fotocasa$hospital_count <- log1p(fotocasa$hospital_count)
fotocasa$university_count <- log1p(fotocasa$university_count)
fotocasa$college_count <- log1p(fotocasa$college_count)
fotocasa$public_transport_count <- log1p(fotocasa$public_transport_count)
fotocasa$priceAmount <- log1p(fotocasa$priceAmount)


# Graficar densidades transformadas

plot(density(fotocasa$priceDescription_rent), main = "precio/80m2 (Ra√≠z)", col = "red3", lwd = 2, xlab = "priceDescription_rent_sqrt")
plot(density(fotocasa$priceDescription_buy), main = "precio/80m2 (Ra√≠z)", col = "brown", lwd = 2, xlab = "priceDescription_buy_sqrt")
plot(density(fotocasa$supermarket_count), main = "supermercados cercanos (Ra√≠z)", col = "blue1", lwd = 2, xlab = "supermarket_count_sqrt")
plot(density(fotocasa$pharmacy_count), main = "farmacias cercanas (Ra√≠z)", col = "green2", lwd = 2, xlab = "pharmacy_count_sqrt")
plot(density(fotocasa$hospital_count), main = "hospitales cercanos (Ra√≠z)", col = "yellow2", lwd = 2, xlab = "hospital_count_sqrt")
plot(density(fotocasa$university_count), main = "universidades cercanas (Ra√≠z)", col = "orange", lwd = 2, xlab = "university_count_sqrt")
plot(density(fotocasa$college_count), main = "colegios cercanos (Ra√≠z)", col = "purple", lwd = 2, xlab = "college_count_sqrt")
plot(density(fotocasa$public_transport_count), main = "Estaciones transporte cercanas (Ra√≠z)", col = "pink2", lwd = 2, xlab = "public_transport_count_sqrt")
plot(density(fotocasa$priceAmount), main = "precio (Ra√≠z)", col = "black", lwd = 2, xlab = "precioAmount_sqrt")

# Restaurar la configuraci√≥n de gr√°ficos a la normalidad
par(mfrow = c(1, 1))

```

```{r undersampling}
# 1. Separar las clases
minority <- fotocasa[fotocasa$binaryDrop == 1, ]
majority <- fotocasa[fotocasa$binaryDrop == 0, ]

# 2. Calcular cu√°ntas observaciones tomar de la clase mayoritaria (2 veces la cantidad de la clase minoritaria)
n_minority <- nrow(minority)
n_majority_sample <- 2 * n_minority

# 3. Submuestreo aleatorio de la clase mayoritaria
set.seed(123)  # Fijar semilla para reproducibilidad
majority_sample <- majority[sample(nrow(majority), n_majority_sample), ]

# 4. Combinar ambos subconjuntos en uno balanceado con proporci√≥n 2:1
fotocasa_undersampled <- rbind(minority, majority_sample)

# 5. Verificar proporci√≥n resultante
kable(table(fotocasa_undersampled$binaryDrop))
```

Una vez seleccionada la base de datos sobre la que aplicaremos el an√°lisis, dividimos las observaciones en un 80% de datos de entrenamiento y 20% de datos de test.

```{r separacion_datos}
set.seed(100)
fotocasa_undersampled$binaryDrop = factor(fotocasa_undersampled$binaryDrop)
trainFilas = createDataPartition(fotocasa_undersampled$binaryDrop, p=0.8, list=FALSE)
head(trainFilas) # trainFilas contiene los n√∫meros de las filas que ir√°n a Train
```


```{r train_test}
trainDatos = fotocasa_undersampled[trainFilas,] 
testDatos = fotocasa_undersampled[-trainFilas,]
num = table(trainDatos$binaryDrop)
perc = round(100*num/sum(num), 2)
kable(cbind(num, perc))
```

Para entrenar el modelo, realizaremos la validaci√≥n cruzada mediante el m√©todo k-folds repetido. Las observaciones se dividir√°n en k=10 grupos y todo el proceso se repetir√° r=30 veces.

```{r kfolds}
myTrainControl = trainControl(method = "repeatedcv",  # k-fold
                              number = 10,  # num folds
                              repeats = 30) # num veces a repetir CV

```



```{r modelo_lda}
set.seed(100)

trainDatosESC <- trainDatos

# Identificar las columnas num√©ricas (excluyendo la variable objetivo)
vars_escalar <- setdiff(names(trainDatos), "binaryDrop")
vars_numericas <- vars_escalar[sapply(trainDatos[, vars_escalar], is.numeric)]

# Escalar solo las columnas num√©ricas
trainDatosESC[, vars_numericas] <- scale(trainDatos[, vars_numericas]) 
# Entrenar el modelo LDA

modeloTR <- train(binaryDrop ~ ., data = trainDatosESC, method = 'lda',
                  trControl = myTrainControl,
                  preProcess = c("zv","pca")  # ‚Üê Esto elimina autom√°ticamente columnas sin varianza en cada fold
                  )
modeloTR 
```


El √≠ndice de Kappa del modelo planteado es relativamente bajo; se√±al de un modelo pobre con poca capacidad predictiva. La variable binaryDrop est√° muy condicionada por el factor humano y las necesidades y/o intereses individuales del propietario. Puede ser que estas caracter√≠sticas la vuelvan una variable de naturaleza impredecible. Sin embargo, queda planteada la idea y no descartamos repetir el an√°lisis en estudios posteriores; con variables m√°s adecuadas para discriminar la bajada de los precios.





```{r intento_mejora}


# 1. Filtrar solo variables num√©ricas
datos_numericos <- trainDatosESC[, sapply(trainDatosESC, is.numeric)]

# 2. Calcular matriz de correlaci√≥n
cor_matrix <- cor(datos_numericos, use = "pairwise.complete.obs")

# 3. Definir umbral de correlaci√≥n
umbral <- 0.6

# 4. Encontrar pares altamente correlacionados (cor > umbral y < 1)
pares_correlados <- which(abs(cor_matrix) > umbral & abs(cor_matrix) < 1, arr.ind = TRUE)

# 5. Evitar duplicados (porque la matriz es sim√©trica)
pares_correlados <- pares_correlados[pares_correlados[,1] < pares_correlados[,2], ]

# 6. Mostrar resultados
cat(" Variables altamente correlacionadas (|cor| >", umbral, "):\n\n")
for (i in seq_len(nrow(pares_correlados))) {
  var1 <- rownames(cor_matrix)[pares_correlados[i, 1]]
  var2 <- colnames(cor_matrix)[pares_correlados[i, 2]]
  cor_val <- cor_matrix[pares_correlados[i, 1], pares_correlados[i, 2]]
  cat(sprintf("üîó %s  ~  %s  |  correlaci√≥n = %.3f\n", var1, var2, cor_val))
}


```







